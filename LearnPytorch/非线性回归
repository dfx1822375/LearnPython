import numpy as np
import matplotlib.pyplot as plt
from torch import nn, optim
from torch.autograd import Variable
import torch

x_data = np.linspace(-2, 2, 200)[:, np.newaxis]  # 产生-2到2的数，并化为2维
noise = np.random.normal(0, 0.2, x_data.shape)
y_data = np.square(x_data) + noise
plt.figure()
plt.subplot(1, 2, 1)
plt.scatter(x_data, y_data)
x_data = x_data.reshape(-1, 1)  # 将数据化为一列任意行
y_data = y_data.reshape(-1, 1)
# 把numpy数据变成tensor\n",
x_data = torch.FloatTensor(x_data)
y_data = torch.FloatTensor(y_data)
inputs = Variable(x_data)
target = Variable(y_data)


# 构建神经网络模型",
# 一般把网络中具有可学习参数的层放在__init__()中


class NLinearRegression(nn.Module):
    # 定义网络结构，函数一般分为初始化函数和计算函数

    def __init__(self):
        # 初始化nn.Module
        super(NLinearRegression, self).__init__()
        # 1-10-1
        self.fc1 = nn.Linear(1, 10)  # 非线性回归需加入隐藏层
        self.tanh = nn.Tanh()        # 设置激活函数
        self.fc2 = nn.Linear(10, 1)  # 全连接层即输出层

    # 定义网络计算

    def forward(self, x):
        x = self.fc1(x)
        x = self.tanh(x)
        x = self.fc2(x)
        return x


# 定义模型


model = NLinearRegression()
# 定义代价函数
mse_loss = nn.MSELoss()
# 定义优化器
optimizer = optim.SGD(model.parameters(), lr=0.3)  # lr为学习率
for name, parameters in model.named_parameters():  # 查看模型参数
    print('name:{}, param:{}'.format(name, parameters))
for i in range(2001):  # 开始训练
    out = model(inputs)
    # 计算loss
    loss = mse_loss(out, target)
    # 梯度清0
    optimizer.zero_grad()
    # 计算梯度
    loss.backward()
    # 修改权值
    optimizer.step()
    if i % 200 == 0:  # 每200次输出代价函数
        print(i, loss.item())
y_pre = model(inputs)
plt.subplot(1, 2, 2)
plt.scatter(x_data, y_data)
plt.plot(x_data, y_pre.data.numpy(), 'r-', lw=3)
plt.show()
